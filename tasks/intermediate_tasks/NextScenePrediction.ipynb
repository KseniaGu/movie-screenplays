{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NextScenePrediction.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "M_MBFXT1i9_b",
        "UzCrPwShjDF4"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rdt6A1zZcVzg"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install mlflow --quiet\n",
        "!pip install pyngrok --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z57pPi_-XA9L"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA1vXcK4cYlM"
      },
      "source": [
        "import os \n",
        "import torch \n",
        "import pickle \n",
        "import pandas as pd \n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from time import time\n",
        "import seaborn as sn\n",
        "from matplotlib import pyplot as plt \n",
        "from torch.nn import LSTM\n",
        "import numpy as np \n",
        "import datetime\n",
        "import random\n",
        "from shutil import copytree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, LongformerTokenizer, \\\n",
        "    LongformerForSequenceClassification, BertForSequenceClassification,AdamW,\\\n",
        "    get_cosine_schedule_with_warmup, BertTokenizerFast, BertModel, BertForNextSentencePrediction\n",
        "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import TensorDataset, RandomSampler, DataLoader\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import torch.nn as nn\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "import torch.optim as Opt\n",
        "#torch.set_default_tensor_type(torch.HalfTensor)\n",
        "#torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
        "import mlflow\n",
        "from pyngrok import ngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqYalta6XEvU"
      },
      "source": [
        "config = {\n",
        "    'paths':{\n",
        "        'script_annotations': '/content/drive/MyDrive/Movie scripts dataset/Movie scripts and annotations/Script annotations by BERT/row_classification',\n",
        "        'ckpt_dir': '/content/drive/MyDrive/Movie scripts models/BERTNSP/ckpts',\n",
        "        'mlruns': '/content/drive/MyDrive/Movie scripts models/BERTNSP/mlruns',\n",
        "        'data_input_ids':'/content/drive/MyDrive/Movie scripts models/BERTNSP/data/tokenized_scripts_input_ids.pickle',\n",
        "        'data_attention_masks':'/content/drive/MyDrive/Movie scripts models/BERTNSP/data/tokenized_scripts_attention_masks.pickle',\n",
        "        'data_token_type_ids':'/content/drive/MyDrive/Movie scripts models/BERTNSP/data/tokenized_scripts_token_type_ids.pickle'\n",
        "    },\n",
        "    'train': {\n",
        "                'optim' : {\n",
        "                    'AdamW':{\n",
        "                        'lr':1e-5,\n",
        "                        'eps': 1e-8,\n",
        "                        'weight_decay':0.0001\n",
        "                    }\n",
        "                },\n",
        "                'embedding_size': 768, \n",
        "                'model': BertForNextSentencePrediction, # from [BertForSequenceClassification, LongformerForSequenceClassification]\n",
        "                'tokenizer':BertTokenizerFast, # from [BertTokenizer, LongformerTokenizer]\n",
        "                'pretrained_model_type': 'bert-base-cased', # from ['bert-base-cased', 'allenai/longformer-base-4096']\n",
        "                #'max_script_length' : 65540, #  131072, 65540, 32770\n",
        "                'max_seq_length' : 512,\n",
        "                'num_classes' : 2,\n",
        "                #'max_scene_number' : 100, \n",
        "                #'nrof_steps' : 2100,\n",
        "                'nrof_epochs' : 100,  \n",
        "                 'tr_batch_size' : 4, #?\n",
        "                'tst_batch_size' : 4, #?\n",
        "                'exp_name':'next_scene_prediction', \n",
        "                'device': torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "                #'heading_to_class_map': {'scene_heading':0, 'text':1, 'speaker_heading':2, 'dialog':3},\n",
        "                #'class_to_heading_map': {0:'scene_heading', 1:'text', 2:'speaker_heading', 3:'dialog'},\n",
        "                'load_model':True\n",
        "                }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzX6_LAtX_zJ"
      },
      "source": [
        "## Dataset preprocessing:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_MBFXT1i9_b"
      },
      "source": [
        "### Main:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i860RjAijhlD"
      },
      "source": [
        "def show_histogram(x_data, y_data, values_to_show=None, figsize=(40, 10), x_label='x', y_label='y',\n",
        "                   set_rotation=False, rotation_angle=45, title='title', file_path='name.png', dpi=100, to_save=False, to_show=True):\n",
        "\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    h = sns.barplot(x=x_data, y=y_data, palette=\"Blues_d\")\n",
        "    if values_to_show:\n",
        "        for i,y in enumerate(y_data):\n",
        "            h.text(i, y, str(y)+'\\n('+str(values_to_show[i]) + ')', color='black', ha='center')\n",
        "    if set_rotation:\n",
        "        h.set_xticklabels(h.get_xticklabels(), rotation=rotation_angle)\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.title(title)\n",
        "    if to_show:\n",
        "        plt.show()\n",
        "\n",
        "    if to_save:\n",
        "        fig.savefig(file_path, dpi=dpi, bbox_inches='tight')\n",
        "        \n",
        "def remove_labels(row):\n",
        "    if row.startswith('text:'):\n",
        "        row = row[6:]\n",
        "    elif row.startswith('dialog:'):\n",
        "        row = row[8:]\n",
        "    elif row.startswith('speaker_heading:'):\n",
        "        row = row[17:]\n",
        "    elif row.startswith('scene_heading:'):\n",
        "        row = row[15:]\n",
        "    return row.strip()\n",
        "\n",
        "def check_if_paths_exist(*paths):\n",
        "    for path in paths:\n",
        "        if not os.path.exists(path):\n",
        "            return False\n",
        "    return True \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBXnASluX-9O"
      },
      "source": [
        "tokenizer = config['train']['tokenizer'].from_pretrained(config['train']['pretrained_model_type'], \n",
        "                                          do_lower_case=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuWpE5EbZes_"
      },
      "source": [
        "class ScriptsData:\n",
        "    def __init__(self, config, tokenizer):\n",
        "        self.config = config \n",
        "        self.tokenizer = tokenizer\n",
        "        self.scripts = self._get_scripts()\n",
        "        self.scripts_scenes = self._get_script_scenes()\n",
        "        self.scene_pairs, self.labels = self._get_scene_pairs()\n",
        "\n",
        "    def _get_scripts(self, to_load=True):\n",
        "        if not (to_load and os.path.exists('scripts.pickle')):\n",
        "            scripts  = []\n",
        "            for file_name in tqdm(os.listdir(self.config['paths']['script_annotations'])):\n",
        "                imdb_id = file_name.split('_')[1] \n",
        "                with open(os.path.join(self.config['paths']['script_annotations'], file_name), 'r') as f:\n",
        "                    anno_lines = f.readlines()\n",
        "                scripts.append(anno_lines)\n",
        "            with open('scripts.pickle', 'wb') as f:\n",
        "                pickle.dump(scripts, f)\n",
        "\n",
        "        else:\n",
        "            with open('scripts.pickle', 'rb') as f:\n",
        "                scripts = pickle.load(f)\n",
        "            print('Data loaded')\n",
        "        return scripts\n",
        "\n",
        "    def _get_script_scenes(self, scene_max_length=20000):\n",
        "        scripts_scenes = []\n",
        "\n",
        "        scene_text = ''\n",
        "        for i, script_lines in tqdm(enumerate(self.scripts)):\n",
        "            scripts_scenes.append([])\n",
        "            for line_num, line in enumerate(script_lines):\n",
        "                if line.startswith('scene_heading:'):\n",
        "                    if scene_text and len(scene_text)<scene_max_length:\n",
        "                        scripts_scenes[-1].append(scene_text)\n",
        "                    scene_text = remove_labels(line)\n",
        "                else:\n",
        "                    scene_text+=remove_labels(line)\n",
        "            if not scripts_scenes[-1]:\n",
        "                scripts_scenes.pop(-1)\n",
        "                \n",
        "        return scripts_scenes\n",
        "\n",
        "    def _get_scene_pairs(self):\n",
        "        scene_pairs, labels = [], []\n",
        "        for script in tqdm(self.scripts_scenes):\n",
        "            for i in range(1, len(script)):\n",
        "                scene_pairs.append((script[i-1], script[i]))\n",
        "                labels.append(1)\n",
        "                if i+2<len(script):\n",
        "                    random_scene_ind = random.choice(list(range(i-1)) + list(range(i+2, len(script))))\n",
        "                    if random_scene_ind > i:\n",
        "                        scene_pairs.append((script[i], script[random_scene_ind]))\n",
        "                    else:\n",
        "                        scene_pairs.append((script[random_scene_ind], script[i]))\n",
        "                    labels.append(0)\n",
        "        return scene_pairs, labels\n",
        "\n",
        "    def tokenize_scripts(self, to_load=True, max_nrof_examples=200000):\n",
        "        if to_load and check_if_paths_exist(self.config['paths']['data_input_ids'],\n",
        "                                            self.config['paths']['data_token_type_ids'],\n",
        "                                            self.config['paths']['data_attention_masks']):\n",
        "            with open(self.config['paths']['data_input_ids'], 'rb') as f:\n",
        "                tokenized_scripts_input_ids = pickle.load(f)\n",
        "            with open(self.config['paths']['data_token_type_ids'], 'rb') as f:\n",
        "                tokenized_scripts_token_type_ids = pickle.load(f)\n",
        "            with open(self.config['paths']['data_attention_masks'], 'rb') as f:\n",
        "                tokenized_scripts_attention_masks = pickle.load(f)\n",
        "            print('tokenized data loaded')\n",
        "        else:\n",
        "            i = 0\n",
        "            tokenized_scripts_input_ids, tokenized_scripts_token_type_ids, tokenized_scripts_attention_masks =[],[],[]\n",
        "            tokenized_scripts = self.tokenizer(self.scene_pairs[:max_nrof_examples],\n",
        "                                                max_length = self.config['train']['max_seq_length'],\n",
        "                                                truncation=True, \n",
        "                                                padding='max_length',\n",
        "                                                return_attention_mask=True,\n",
        "                                                return_tensors='pt')\n",
        "            tokenized_scripts_input_ids = tokenized_scripts['input_ids']\n",
        "            tokenized_scripts_token_type_ids = tokenized_scripts['token_type_ids']\n",
        "            tokenized_scripts_attention_masks=tokenized_scripts['attention_mask']\n",
        "            \n",
        "            with open(self.config['paths']['data_input_ids'], 'wb') as f:\n",
        "                pickle.dump(tokenized_scripts_input_ids, f)\n",
        "            with open(self.config['paths']['data_token_type_ids'], 'wb') as f:\n",
        "                pickle.dump(tokenized_scripts_token_type_ids, f)\n",
        "            with open(self.config['paths']['data_attention_masks'], 'wb') as f:\n",
        "                pickle.dump(tokenized_scripts_attention_masks, f)\n",
        "        return tokenized_scripts_input_ids, tokenized_scripts_token_type_ids, tokenized_scripts_attention_masks\n",
        "    '''\n",
        "    def prepare_tokenized_chunks_masks_labels(self, \n",
        "                                              input_ids, \n",
        "                                              token_type_ids, \n",
        "                                              attention_masks,\n",
        "                                              labels):\n",
        "        input_ids = [torch.tensor(chunk) for chunk in tokenized_script_chunks]\n",
        "        attention_masks = [torch.tensor(mask) for mask in attention_masks]\n",
        "        padded_tokenized_script_chunks = pad_sequence(tokenized_script_chunks, \n",
        "                                                      padding_value=0, batch_first=True)# for BERT pad = 0 ?\n",
        "        padded_attention_masks = pad_sequence(attention_masks, \n",
        "                                              padding_value=0, batch_first=True)\n",
        "        labels = torch.LongTensor(labels)\n",
        "\n",
        "        return padded_tokenized_script_chunks, padded_attention_masks, labels\n",
        "    '''\n",
        "\n",
        "    def get_train_val_split(self):\n",
        "        input_ids, token_type_ids, attention_masks = self.tokenize_scripts()\n",
        "        tensor_labels = torch.LongTensor(self.labels[:200000])\n",
        "        tr_inputs, val_inputs, tr_masks, val_masks, tr_token_type_ids, val_token_type_ids, tr_labels, val_labels = train_test_split(\n",
        "            input_ids, attention_masks, token_type_ids, tensor_labels, \n",
        "            test_size=0.2, random_state=11)\n",
        "\n",
        "        tst_inputs, val_inputs, tst_masks, val_masks, tst_token_type_ids, val_token_type_ids, tst_labels, val_labels = train_test_split(\n",
        "            val_inputs, val_masks, val_token_type_ids, val_labels, \n",
        "            test_size=0.25, random_state=11)\n",
        "        '''\n",
        "        tr_inputs, tr_masks, tr_labels = self.prepare_tokenized_chunks_masks_labels(\n",
        "            tr_inputs, tr_masks, tr_labels)\n",
        "        val_inputs, val_masks, val_labels = self.prepare_tokenized_chunks_masks_labels(\n",
        "            val_inputs, val_masks, val_labels)\n",
        "        tst_inputs, tst_masks, tst_labels = self.prepare_tokenized_chunks_masks_labels(\n",
        "            tst_inputs, tst_masks, tst_labels)\n",
        "        '''\n",
        "        return tr_inputs, tr_masks, tr_token_type_ids, tr_labels, val_inputs, val_masks, val_token_type_ids, val_labels, tst_inputs, tst_masks, tst_token_type_ids, tst_labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meInUFUErFt6"
      },
      "source": [
        "def get_dataloader(input_ids, attention_masks, token_type_ids, labels, batch_size=1,  \n",
        "                   phase='train', sampler=None):\n",
        "        dataset = TensorDataset(input_ids, attention_masks, token_type_ids, labels)\n",
        "        if phase=='train':\n",
        "            sampler = sampler if not sampler is None else RandomSampler(dataset)\n",
        "            dataloader = DataLoader(\n",
        "                        dataset,  \n",
        "                        batch_size = batch_size,\n",
        "                        sampler = sampler,\n",
        "                        drop_last=True\n",
        "                    )\n",
        "        else:\n",
        "            dataloader = DataLoader(\n",
        "                        dataset,  \n",
        "                        batch_size = batch_size,\n",
        "                        drop_last=True\n",
        "                    )\n",
        "\n",
        "        return dataloader "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUK2_WUcrHnq"
      },
      "source": [
        "def get_data_loaders():\n",
        "    SD = ScriptsData(config, tokenizer)\n",
        "    tr_inputs, tr_attention_masks, tr_token_type_ids, tr_labels, val_inputs, val_attention_masks, \\\n",
        "    val_token_type_ids, val_labels, tst_inputs, tst_attention_masks, tst_token_type_ids, tst_labels = SD.get_train_val_split()\n",
        "\n",
        "    #tr_loader = get_dataloader(tr_inputs[[0, 1, 3, 4]], tr_labels[[0, 1, 3, 4]],  tr_attention_masks[[0, 1, 3, 4]], # [0, 1, 3, 4]\n",
        "    tr_loader = get_dataloader(tr_inputs, tr_attention_masks, tr_token_type_ids,tr_labels,\n",
        "                            batch_size=config['train']['tr_batch_size'])\n",
        "    val_loader = get_dataloader(val_inputs, val_attention_masks, val_token_type_ids, val_labels,\n",
        "                                batch_size=config['train']['tst_batch_size'])\n",
        "    tst_loader = get_dataloader(tst_inputs, tst_attention_masks, tst_token_type_ids, tst_labels,\n",
        "                                batch_size=config['train']['tst_batch_size'])\n",
        "    \n",
        "    return tr_loader, val_loader, tst_loader\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzCrPwShjDF4"
      },
      "source": [
        "### Check:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpnolA1DcEEU"
      },
      "source": [
        "SD = ScriptsData(config, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAxI7i7uYcKE"
      },
      "source": [
        "attention_masks, input_ids, token_type_ids = [], [], []\n",
        "\n",
        "for i in range(1,5):\n",
        "    with open('/content/tokenized_scripts_attention_masks_' + str(i) + '.pickle', 'rb') as f:\n",
        "        attention_masks.append(pickle.load(f))\n",
        "\n",
        "    with open('/content/tokenized_scripts_input_ids_' + str(i) + '.pickle', 'rb') as f:\n",
        "        input_ids.append(pickle.load(f))\n",
        "\n",
        "    with open('/content/tokenized_scripts_token_type_ids_' + str(i) + '.pickle', 'rb') as f:\n",
        "        token_type_ids.append(pickle.load(f))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZeXYOhic0Xm"
      },
      "source": [
        "tr_inputs, tr_masks, tr_token_type_ids, tr_labels, val_inputs, val_masks, val_type_ids, val_labels, tst_inputs, tst_masks, tst_type_ids, tst_labels = SD.get_train_val_split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dre17GTUedzo"
      },
      "source": [
        "print(tr_inputs.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IyNbsCipDrs"
      },
      "source": [
        "tokenizer.convert_ids_to_tokens([101, 157, 10964, 12426, 1592, 22219, 2036, 2924, 2036, 2924, 7729, 5208, 1118, 2107, 24554, 1161, 139, 9435, 4729, 2064, 6530, 1181, 1113, 1103, 1520, 1118, 2101, 28114, 14159, 6262, 16838, 1116, 10973, 1582, 1357, 1371, 102])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEpuxjUfesdL"
      },
      "source": [
        "tr_loader, val_loader, tst_loader = get_data_loaders()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1IQe71LhXxR"
      },
      "source": [
        "for d in tr_loader:\n",
        "    print(d)\n",
        "    print(d[0].size())\n",
        "    print(d[1].size())\n",
        "    print(d[2].size())\n",
        "    print(d[3].size())\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9WT7FcUrAUV"
      },
      "source": [
        "## Training process:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeFLRfDWN29l"
      },
      "source": [
        "def plot_conf_matr(trues, predicts, title='tr_', \n",
        "                   classes_names=['next', 'not next'],\n",
        "                   nrof_classes=2):\n",
        "    results = np.zeros((nrof_classes, nrof_classes))\n",
        "    for t, p in zip(trues, predicts):\n",
        "        results[t][p]+=1\n",
        "\n",
        "    df_cm = pd.DataFrame(results.astype(np.int), index = classes_names,\n",
        "                  columns = classes_names)\n",
        "    plt.figure(figsize = (7,7))    \n",
        "    ax = sns.heatmap(df_cm, annot=True, fmt='d')\n",
        "    ax.set(xlabel='predicted', ylabel='actual',title='Confusion matrix')\n",
        "    plt.savefig(title+'conf_matrix.png', bbox_inches='tight')\n",
        "    plt.close()\n",
        "    \n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cph-4S5dKH4G"
      },
      "source": [
        "class Train():\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.model = config['train']['model'].from_pretrained(config['train']['pretrained_model_type'], \n",
        "                                          num_labels = self.config['train']['num_classes'], \n",
        "                                          output_attentions = False, \n",
        "                                          output_hidden_states = False)\n",
        "\n",
        "        opt_config = self.config['train']['optim']['AdamW']\n",
        "        #for key, val in opt_config.items():\n",
        "         #   mlflow.log_param(key, val)\n",
        "        #mlflow.log_param('nrof_classes', self.config['train']['num_classes'])\n",
        "        #self.total_steps = self.config['train']['nrof_steps']\n",
        "        self.optimizer = AdamW(self.model.parameters(),\n",
        "                  lr = opt_config['lr'], \n",
        "                  eps = opt_config['eps'], \n",
        "                  weight_decay=opt_config['weight_decay'],\n",
        "                )        \n",
        "        #self.scheduler = get_cosine_schedule_with_warmup(self.optimizer, \n",
        "         #                                   num_warmup_steps = 10, \n",
        "          #                                  num_training_steps = self.total_steps)\n",
        "        self.model.to(self.config['train']['device'])\n",
        "        self.global_step = 0\n",
        "        self.max_steps_to_stop = 1000\n",
        "        self.best_val_loss = 1000\n",
        "        \n",
        "    \n",
        "    def save_model(self):\n",
        "        torch.save({\"model\": self.model.state_dict(),\n",
        "                    \"optimizer\": self.optimizer.state_dict(),\n",
        "                    'global_step': self.global_step\n",
        "                    #\"scheduler\": self.scheduler.state_dict(),\n",
        "                    },\n",
        "                   os.path.join(self.config['paths']['ckpt_dir'], \n",
        "                                self.config['train']['exp_name'] + '_checkpoint'))\n",
        "    \n",
        "    def load_model(self):\n",
        "        ckpt = torch.load(os.path.join(self.config['paths']['ckpt_dir'],\n",
        "                                       self.config['train']['exp_name'] + '_checkpoint'),\n",
        "                          map_location=self.config['train']['device'])\n",
        "        model_st_dict = ckpt[\"model\"]\n",
        "        self.global_step = ckpt[\"global_step\"] + 1\n",
        "        self.model.load_state_dict(model_st_dict)   \n",
        "        self.optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "        #self.scheduler.load_state_dict(ckpt[\"scheduler\"])\n",
        "        print(\"Model loaded...\")\n",
        "\n",
        "\n",
        "    def train(self, train_dataloader, validation_dataloader, to_save=True):\n",
        "        if self.config['train']['load_model']:\n",
        "            self.load_model()\n",
        "        self.model.train()\n",
        "        t0 = time()\n",
        "        \n",
        "        cur_loss, nrof_steps, = 0., 0,\n",
        "        nrof_steps_to_stop = 0\n",
        "        nrof_cor_predicts, nrof_samples = 0, 0\n",
        "        predicts, trues = [], []\n",
        "        print('Global step:', self.global_step)\n",
        "        #nrof_cor_predicts_current, nrof_samples_current = 0, 0\n",
        "        #class_correct = list(0. for i in range(self.config['train']['num_classes']))\n",
        "        #class_total = list(0. for i in range(self.config['train']['num_classes']))\n",
        "        \n",
        "\n",
        "        for epoch in tqdm(range(self.config['train']['nrof_epochs'])):\n",
        "            predicts, trues = [], []\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "                if epoch * len(train_dataloader) + step < self.global_step:\n",
        "                    continue\n",
        "                print('epoch: {} step: {}'.format(epoch, step))\n",
        "                b_input_ids = batch[0].to(self.config['train']['device'])\n",
        "                b_input_mask = batch[1].to(self.config['train']['device'])\n",
        "                b_token_type_ids = batch[2].to(self.config['train']['device'])\n",
        "                b_labels = batch[3].to(self.config['train']['device'])\n",
        "                \n",
        "                self.model.zero_grad()  \n",
        "\n",
        "                outputs = self.model(b_input_ids,  \n",
        "                                    b_input_mask,\n",
        "                                     b_token_type_ids,\n",
        "                                    labels=b_labels, #.unsqueeze(0),\n",
        "                                    return_dict=True)\n",
        "                #print(outputs)\n",
        "                #loss = self.crit(output.unsqueeze(0), b_labels)\n",
        "                cur_loss += outputs.loss.item()\n",
        "                #cur_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.logits,-1)\n",
        "                predicts.extend(predicted.cpu().detach().numpy().tolist())\n",
        "                trues.extend(b_labels.cpu().detach().numpy().tolist())\n",
        "                if_right = (predicted == b_labels).sum().item()\n",
        "                nrof_cor_predicts += if_right\n",
        "                #nrof_cor_predicts_current += if_right\n",
        "                #print('class_correct', class_correct)\n",
        "                #print('if_right', if_right)\n",
        "                #for i, label in enumerate(b_labels.cpu().detach().numpy()):\n",
        "                    #print(label)\n",
        "                    #print(int(label))\n",
        "                 #   class_correct[int(label)] += (predicted == b_labels)[i].item()\n",
        "                  #  class_total[int(label)] += 1\n",
        "\n",
        "                #nrof_cor_predicts += if_right\n",
        "                #nrof_cor_predicts_current += if_right\n",
        "                #class_correct[b_labels.item()] += if_right\n",
        "                #class_total[b_labels.item()] += len(b_labels)\n",
        "\n",
        "                outputs.loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "                self.optimizer.step()\n",
        "                #self.scheduler.step()\n",
        "                self.global_step+=1\n",
        "                nrof_steps+=1\n",
        "                nrof_samples+=len(b_labels)\n",
        "                #nrof_samples_current+=len(b_labels)\n",
        "\n",
        "                \n",
        "                if self.global_step % 10 == 0: \n",
        "                    if self.global_step % 100 == 0: \n",
        "                        avg_val_loss, val_trues, val_predicts = self.validate(validation_dataloader)\n",
        "                        avg_val_accuracy = balanced_accuracy_score(val_trues, val_predicts)\n",
        "                        #val_f1_score = f1_score(val_trues, val_predicts)\n",
        "                        mlflow.log_metric(\"val_loss\", avg_val_loss, step = self.global_step)\n",
        "                        mlflow.log_metric(\"val_accuracy\", avg_val_accuracy, step = self.global_step)\n",
        "                        #mlflow.log_metric(\"val_f1_score\", val_f1_score, step = self.global_step)\n",
        "                        print('val loss: {}\\nval accuracy: {}'.format(avg_val_loss,  avg_val_accuracy))\n",
        "                        \n",
        "                        plot_conf_matr(val_trues, val_predicts, title=str(self.global_step)+'_', \n",
        "                                    classes_names=['not next', 'next'])\n",
        "                        mlflow.log_artifact(str(self.global_step)+'_conf_matrix.png')\n",
        "\n",
        "                        if avg_val_loss < self.best_val_loss:\n",
        "                            self.best_val_loss = avg_val_loss\n",
        "                            self.save_model()\n",
        "                            try:\n",
        "                                copytree('/content/mlruns', self.config['paths']['mlruns'] + '_' + str(self.global_step))\n",
        "                            except Exception as e:\n",
        "                                print('Exception {} on step {}'.format(e, self.global_step))\n",
        "                                pass \n",
        "                            #if nrof_steps_to_stop>0:\n",
        "                             #   nrof_steps_to_stop -= 1\n",
        "                        else:\n",
        "                            pass \n",
        "                            #nrof_steps_to_stop+=1\n",
        "                            #if nrof_steps_to_stop > self.max_steps_to_stop:\n",
        "                             #   break\n",
        "                              #  try:\n",
        "                               #     copytree(+str(self.global_step))\n",
        "                               # except Exception as e:\n",
        "                               # print(e)\n",
        "                        self.model.train()\n",
        "                        torch.cuda.empty_cache()\n",
        "                    predicts, trues = predicts[-50:], trues[-50:]\n",
        "                    elapsed = format_time(time() - t0)\n",
        "                    print('Elapsed: {:}.'.format(elapsed))\n",
        "                    \n",
        "                    mlflow.log_metric(\"train_loss\", cur_loss / nrof_steps, step = self.global_step)\n",
        "                    #tr_class_accs = np.asarray(class_correct) / np.asarray(class_total)\n",
        "                    #tr_bal_acc = np.mean(tr_class_accs)\n",
        "                    #mlflow.log_metric(\"train_accuracy\", float(nrof_cor_predicts_current)/nrof_samples_current)\n",
        "                    #mlflow.log_metric(\"train_accuracy\", float(nrof_cor_predicts)/nrof_samples)\n",
        "                    tr_accuracy = balanced_accuracy_score(trues, predicts)\n",
        "                    #tr_f1_score = f1_score(trues, predicts)\n",
        "                    mlflow.log_metric(\"train_accuracy\", tr_accuracy, step = self.global_step)\n",
        "                    #mlflow.log_metric(\"f1_score\", tr_f1_score, step = self.global_step)\n",
        "                    mlflow.log_metric(\"learning_rate\", self.optimizer.state_dict()[\"param_groups\"][0][\"lr\"])\n",
        "                    \n",
        "                    #mlflow.log_metric(\"train_balanced_accuracy\", tr_bal_acc)\n",
        "                    print('tr loss: {}\\ntr accuracy: {}'.format(cur_loss/nrof_steps,  tr_accuracy))\n",
        "                    print()\n",
        "\n",
        "                    cur_loss, nrof_steps, nrof_samples, nrof_cor_predicts = 0., 0, 0, 0\n",
        "\n",
        "                \n",
        "                    #if self.global_step % 20 == 0: # 5\n",
        "                    '''\n",
        "                    if avg_val_loss < self.best_val_loss:\n",
        "                        self.best_val_loss = avg_val_loss\n",
        "                        self.save_model()\n",
        "                        if nrof_steps_to_stop>0:\n",
        "                            nrof_steps_to_stop -= 1\n",
        "                    else:\n",
        "                        nrof_steps_to_stop+=1\n",
        "                        if nrof_steps_to_stop > self.max_steps_to_stop:\n",
        "                            break\n",
        "                            try:\n",
        "                               copytree(+str(self.global_step))\n",
        "                            except Exception as e:\n",
        "                               print(e)\n",
        "                    '''       \n",
        "                    \n",
        "                    \n",
        "            #if nrof_steps_to_stop > self.max_steps_to_stop:\n",
        "             #   break\n",
        "        \n",
        "        '''\n",
        "        self.save_model()\n",
        "        try:\n",
        "            copytree('/content/mlruns', '/content/drive/MyDrive/CharEval/mlruns')\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            pass \n",
        "        '''\n",
        "            \n",
        "\n",
        "\n",
        "    def validate(self, validation_dataloader, to_load=False):\n",
        "        if to_load:\n",
        "            self.load_model()\n",
        "        t1 = time()\n",
        "        self.model.eval()\n",
        "        predicts, trues = [], []\n",
        "        nrof_steps, val_loss, nrof_cor_predicts, nrof_samples = 0, 0., 0, 0\n",
        "        #class_correct = list(0. for i in range(self.config['train']['num_classes']))\n",
        "        #class_total = list(0. for i in range(self.config['train']['num_classes']))\n",
        "\n",
        "        with torch.no_grad(): \n",
        "            for i, batch in tqdm(enumerate(validation_dataloader)):\n",
        "                if i>10000:\n",
        "                    break\n",
        "                b_input_ids = batch[0].to(self.config['train']['device'])\n",
        "                b_input_mask = batch[1].to(self.config['train']['device'])\n",
        "                b_token_type_ids = batch[2].to(self.config['train']['device'])\n",
        "                b_labels = batch[3].to(self.config['train']['device'])\n",
        "\n",
        "                output = self.model(b_input_ids,  \n",
        "                                    b_input_mask,\n",
        "                                     b_token_type_ids,\n",
        "                                    labels=b_labels, #.unsqueeze(0),\n",
        "                                    return_dict=True)\n",
        "                            \n",
        "                val_loss += output.loss.item()\n",
        "                #val_loss += self.crit(output.unsqueeze(0), b_labels)\n",
        "                _, predicted = torch.max(output.logits,-1)\n",
        "                predicts.extend(predicted.cpu().detach().numpy().tolist())\n",
        "                trues.extend(b_labels.cpu().detach().numpy().tolist())\n",
        "                if_right = (predicted == b_labels).sum().item()\n",
        "                nrof_cor_predicts += if_right\n",
        "                #nrof_cor_predicts_current += if_right\n",
        "                #print('class_correct', class_correct)\n",
        "                #print('if_right', if_right)\n",
        "                #for i, label in enumerate(b_labels.cpu().detach().numpy()):\n",
        "                    #print(label)\n",
        "                    #print(int(label))\n",
        "                 #   class_correct[int(label)] += (predicted == b_labels)[i].item()\n",
        "                  #  class_total[int(label)] += 1\n",
        "\n",
        "                nrof_steps+=1\n",
        "                nrof_samples+=len(b_labels)\n",
        "                #nrof_samples_current+=len(b_labels)\n",
        "                \n",
        "                \n",
        "        #avg_val_accuracy = nrof_cor_predicts / nrof_samples\n",
        "        #avg_val_accuracy = float((np.asarray(class_correct) / np.asarray(class_total)).sum())\n",
        "        #avg_val_accuracy = balanced_accuracy_score(trues, predicts)\n",
        "        #val_f1_score = f1_score(trues, predicts)\n",
        "        #val_classes_accs = np.asarray(class_correct) / np.asarray(class_total)\n",
        "        avg_val_loss = val_loss / nrof_steps\n",
        "        validation_time = (time() - t1)\n",
        "        \n",
        "        print(\"  Validation took: {:}\".format(validation_time))\n",
        "        \n",
        "        return avg_val_loss, trues, predicts\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9ZY51XR9Q3C"
      },
      "source": [
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "tr_loader, val_loader, tst_loader = get_data_loaders()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQPyQNovBDBF"
      },
      "source": [
        "print(len(tr_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGWxGAZHA__Q"
      },
      "source": [
        "T = Train(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCazr5AyIiE4"
      },
      "source": [
        "avg_tst_loss, tst_trues, tst_predicts = T.validate(tst_loader, to_load=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_FxJeDlNlfc"
      },
      "source": [
        "balanced_accuracy_score(tst_trues, tst_predicts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9s8LCRdQQdr"
      },
      "source": [
        "plot_conf_matr(trues, predicts, title='bert_nsp_test_')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HFogLcdA5jP"
      },
      "source": [
        "avg_tr_loss, trues, predicts = T.validate(tr_loader, to_load=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2iNqJLgH50O"
      },
      "source": [
        "print(avg_tr_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2epmuJQIT9n"
      },
      "source": [
        "balanced_accuracy_score(trues, predicts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Amfa5_hAH7_N"
      },
      "source": [
        "plot_conf_matr(trues, predicts, title='bert_nsp_train_')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cox5XW8kgnZx"
      },
      "source": [
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "with mlflow.start_run(run_name=config['train']['exp_name'], run_id='b0dbc31ddd184269b188097712a888a0'):\n",
        "    tr_loader, val_loader, tst_loader = get_data_loaders()\n",
        "    T = Train(config)\n",
        "    T.train(tr_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9gFTiO7iPLi"
      },
      "source": [
        "import shutil\n",
        "\n",
        "shutil.rmtree('/content/mlruns')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U9BVUgMjsRU"
      },
      "source": [
        "!cp -r '/content/drive/MyDrive/Movie scripts models/BERTNSP/mlruns_800' . "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaludEdRiNwP"
      },
      "source": [
        "get_ipython().system_raw(\"mlflow ui --port 5000 &\") # run tracking UI in the background\n",
        "NGROK_AUTH_TOKEN = \"\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Open an HTTPs tunnel on port 5000 for http://localhost:5000\n",
        "ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n",
        "print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz9sOf-giOQo"
      },
      "source": [
        "ngrok.kill()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}